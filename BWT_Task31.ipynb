{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f02cf4f",
   "metadata": {},
   "source": [
    "# BWT - Deep Learning Track\n",
    "## Task#31: Applying One-Hot Encoding and Word Embeddings\n",
    "### Adil Mubashir Chaudhry\n",
    "\n",
    "#### One-Hot Encoding:\n",
    "One-hot encoding is a simple method to represent categorical data, including words in text. In this technique, each word is represented as a binary vector, where the length of the vector is equal to the vocabulary size. The vector has a value of 1 at the index corresponding to the word's position in the vocabulary, and 0 everywhere else. For example, if we have a vocabulary of 10 words, the word \"apple\" would be represented as [0, 0, 0, 0, 0, 0, 0, 0, 1, 0].\n",
    "\n",
    "One-hot encoding has some limitations. It creates high-dimensional sparse vectors, making it computationally expensive and inefficient for large vocabularies. Additionally, it does not capture any semantic relationships between words.\n",
    "\n",
    "#### Word Embeddings:\n",
    "Word embeddings are dense vector representations that capture the semantic meaning and relationships between words. Unlike one-hot encoding, word embeddings are learned from data using techniques like Word2Vec, GloVe, or BERT. These models analyze large text corpora and learn to represent words in a continuous vector space, where similar words are closer to each other in the space.\n",
    "\n",
    "Word embeddings have several advantages. They capture semantic relationships, allowing for meaningful arithmetic operations such as \"king - man + woman = queen.\" They also handle out-of-vocabulary words by providing meaningful representations based on their context. Furthermore, word embeddings reduce the dimensionality of the representation, making them more computationally efficient compared to one-hot encoding.\n",
    "\n",
    "Word embeddings have become a standard approach in natural language processing tasks such as text classification, named entity recognition, machine translation, and sentiment analysis, as they enable models to effectively understand and process textual data by leveraging semantic information encoded in the vectors.\n",
    "\n",
    "Below is a simple example of how word embeddings and one hot encoding can be applied to a deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acf60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca214ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "141/141 [==============================] - 12s 72ms/step - loss: 2.5827 - accuracy: 0.3483 - val_loss: 2.3374 - val_accuracy: 0.3620\n",
      "Epoch 2/3\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 2.0766 - accuracy: 0.4844 - val_loss: 1.8870 - val_accuracy: 0.5227\n",
      "Epoch 3/3\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 1.7695 - accuracy: 0.5470 - val_loss: 1.7687 - val_accuracy: 0.5525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e591de4040>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "\n",
    "max_len = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "embedding_size = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=max_len))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
